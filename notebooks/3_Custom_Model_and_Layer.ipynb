{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models and Layers\n",
    "\n",
    "Here we introduce how to write custom models and layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Model class\n",
    "\n",
    "Recall the earlier example of training a linear regression model in [notbook 1](./1_Tensors_Variables_Operations_and_AutoDiff.ipynb). In cell 24 we **defined and initialized the variables of the model**, and we also **specified the model's forward pass**(ie, given inputs, how the model computes the outputs using its variables.) \n",
    "\n",
    "These two can be better organized in a `model` class by subclass `tf.keras.Model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_weights = tf.constant([1,2,3,4,5], dtype=tf.float32)[:, tf.newaxis]\n",
    "x = tf.constant(tf.random.uniform((5, 5)), dtype=tf.float32)\n",
    "y = tf.constant(x @ true_weights, dtype=tf.float32)\n",
    "\n",
    "LR = .1\n",
    "MAX_EPOCHS = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With custom model class, we initialize the variables of the model in the `__init__` constructor and implement the forward pass in the `call` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(LinearModel, self).__init__(**kwargs)\n",
    "        self.w = tf.Variable(tf.random.uniform((input_dim, output_dim)), dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the variables of the model throught the `.variables` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
       " array([[0.13378656],\n",
       "        [0.1457566 ],\n",
       "        [0.3716122 ],\n",
       "        [0.13667178],\n",
       "        [0.28042758]], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearModel(input_dim=5, output_dim=1)    \n",
    "model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do the forward pass on data with the model object, just as if the model is a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.44342452]\n",
      " [0.29675412]\n",
      " [0.55513775]\n",
      " [0.69912523]\n",
      " [0.34083742]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "o = model(x)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model object, we need to modify the previous training loop to train it. Note that both `model.variables` and `gradients` are lists, so we are doing updates in a loop. \n",
    "\n",
    "This looping over the coupling of variables and gradients weill be more clear with next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, model, learning_rate=LR, max_epochs=MAX_EPOCHS, verbose=0):\n",
    "    for it in range(max_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = model(x)\n",
    "            loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "        if verbose and not (it % verbose): \n",
    "            print('mse loss at iteration {} is {:5.4f}'.format(it, loss))\n",
    "        gradients = tape.gradient(loss, model.variables)\n",
    "        for variables, grads in zip(model.variables, gradients):\n",
    "            variables.assign_add(-learning_rate * grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we recovered the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
       " array([[1.0523298],\n",
       "        [2.1180644],\n",
       "        [2.920983 ],\n",
       "        [3.9075792],\n",
       "        [4.990562 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(x, y, model)\n",
    "model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a bit complexity, we now add a bias term to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(LinearModel, self).__init__(**kwargs)\n",
    "        self.w = tf.Variable(tf.random.uniform((input_dim, output_dim)), dtype=tf.float32)\n",
    "        self.b = tf.Variable(0, dtype=tf.float32)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[1.099868 ],\n",
      "       [2.1637936],\n",
      "       [2.9128673],\n",
      "       [3.8972185],\n",
      "       [5.0019126]], dtype=float32)>,\n",
      " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.044665385>]\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(input_dim=5, output_dim=1)    \n",
    "train(x, y, model, learning_rate=.1, max_epochs=2000, verbose=0)\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see the training code works just fine, with now the model has two sets of variables(as a result two sets of gradients). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer class  \n",
    "\n",
    "The `Model` class is usually for the overall computation which combines many repeated small computational modules. It is ok to code up the small modules with `tf.keras.Model` and then combine them, but its better to use the `Layer` class. \n",
    "\n",
    "Lets say that we want to upgrade the linear regression model to be a composition of two linear transformations. Here is how we utilize the `Layer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units))\n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(shape=(self.units), initializer=\"zeros\")\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = tf.matmul(x, self.w)\n",
    "        if self.use_bias:\n",
    "            output += self.b\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the `Layer` class, we actually creates the Variables in the `build` method, rather than the constructor. This method is called the first time there are data going through the layer, which allows dynamic determination of the sizes of the variables. This makes `layers` more ideal when you try to combining them. \n",
    "\n",
    "Now, lets stack up some layers to make a bigger model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self, num_units, use_bias=True, **kwargs):\n",
    "        super(LinearModel, self).__init__(**kwargs)\n",
    "        self.model = [Linear(units, use_bias) for units in num_units]\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the variables of the model before and after we pass some actual data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "---line break---\n",
      "[<tf.Variable 'linear_model_2/linear/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.2700736 , -0.11668235,  0.44956177],\n",
      "       [ 0.64381224,  0.6111501 ,  0.40530437],\n",
      "       [ 0.50504965,  0.6354911 , -0.8027457 ],\n",
      "       [ 0.43543607,  0.46089858, -0.50693005],\n",
      "       [ 0.4396996 ,  0.81468815, -0.20702368]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_2/linear/Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_2/linear_1/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[-0.01050568],\n",
      "       [-0.8907435 ],\n",
      "       [ 0.3596264 ]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_2/linear_1/Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "tf.Tensor(\n",
      "[[-1.1694874]\n",
      " [-0.4110697]\n",
      " [-1.1702682]\n",
      " [-1.7282639]\n",
      " [-0.4201784]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(num_units=(3,1), use_bias=True)\n",
    "print(model.variables)\n",
    "print('---line break---')\n",
    "o = model(x)\n",
    "pprint(model.variables)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model with many linear layers stacked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss at iteration 0 is 46.6503\n",
      "mse loss at iteration 200 is 0.0002\n",
      "mse loss at iteration 400 is 0.0001\n",
      "mse loss at iteration 600 is 0.0001\n",
      "mse loss at iteration 800 is 0.0000\n",
      "mse loss at iteration 1000 is 0.0000\n",
      "mse loss at iteration 1200 is 0.0000\n",
      "mse loss at iteration 1400 is 0.0000\n",
      "mse loss at iteration 1600 is 0.0000\n",
      "mse loss at iteration 1800 is 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(num_units=(4,3,2,1), use_bias=False)\n",
    "train(x, y, model, learning_rate=0.01, max_epochs=2000, verbose=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'linear_model_8/linear_14/Variable:0' shape=(5, 4) dtype=float32, numpy=\n",
      "array([[-0.30366537, -0.16996984, -0.637681  ,  0.37072608],\n",
      "       [-0.87186205,  0.07727206,  0.5329781 , -0.46546325],\n",
      "       [-0.6101041 , -0.50876856, -0.6042015 ,  0.04365033],\n",
      "       [ 0.15343004, -1.0391084 , -0.10087277, -0.88128865],\n",
      "       [-0.8673619 , -0.14706558, -0.4924126 , -0.833977  ]],\n",
      "      dtype=float32)>,\n",
      " <tf.Variable 'linear_model_8/linear_15/Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.3303018 ,  0.9024163 ,  0.4269654 ],\n",
      "       [-0.18246803,  0.56032246,  0.7382356 ],\n",
      "       [ 0.00510553,  0.81914425,  0.09659762],\n",
      "       [ 0.6227235 ,  0.97610795,  0.51054686]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_8/linear_16/Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[ 0.66775626, -0.23800902],\n",
      "       [ 0.4104188 ,  1.0111378 ],\n",
      "       [ 1.2080138 , -0.12563725]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_8/linear_17/Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-1.0662692],\n",
      "       [-1.402336 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem to be a bit more interesting than our old linear regression model at first glance. But composition of linear transformations is still a linear transformation. With all these layers, we are essentially doing the same linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.9889582]\n",
      " [1.9750128]\n",
      " [3.0168242]\n",
      " [4.0194426]\n",
      " [5.0019574]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(5.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "reduced_model = reduce(tf.matmul, model.variables)\n",
    "print(reduced_model)\n",
    "print(tf.reduce_sum(tf.cast((model(x) - x @ reduced_model) < 1e-5, tf.float32)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
