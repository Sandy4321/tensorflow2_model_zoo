{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models and Layers\n",
    "\n",
    "Here we introduce how to write custom models and layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pprint import pprint\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Model class\n",
    "\n",
    "Recall the earlier example of training a linear regression model in [notbook 1](./1_Tensors_Variables_Operations_and_AutoDiff.ipynb). In cell 24 we **defined and initialized the variables of the model**, and we also **specified the model's forward pass**(ie, given inputs, how the model computes the outputs using its variables.) \n",
    "\n",
    "These two can be better organized in a `model` class by subclass `tf.keras.Model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_weights = tf.constant([1,2,3,4,5], dtype=tf.float32)[:, tf.newaxis]\n",
    "x = tf.constant(tf.random.uniform((32, 5)), dtype=tf.float32)\n",
    "y = tf.constant(x @ true_weights, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With custom model class, we initialize the variables of the model in the `__init__` constructor and implement the forward pass in the `call` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(LinearModel, self).__init__(**kwargs)\n",
    "        self.w = tf.Variable(tf.random.uniform((input_dim, output_dim)), dtype=tf.float32)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the variables of the model throught the `.variables` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
       " array([[0.7385192 ],\n",
       "        [0.09546971],\n",
       "        [0.5498091 ],\n",
       "        [0.6976471 ],\n",
       "        [0.6082227 ]], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearModel(input_dim=5, output_dim=1)    \n",
    "model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do the forward pass on data with the model object, just as if the model is a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.7299302]\n",
      " [1.3127615]\n",
      " [1.4112923]\n",
      " [1.5546852]\n",
      " [1.3384575]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "o = model(x)\n",
    "print(o[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model object, we need to modify the previous training loop to train it. Note that both `model.variables` and `gradients` are lists, so we are doing updates in a loop. \n",
    "\n",
    "This looping over the coupling of variables and gradients weill be more clear with next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, model, learning_rate=.01, max_epochs=10000, min_tol=1e-3, verbose=1000):\n",
    "    best_loss = None\n",
    "    for it in range(max_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_hat = model(x)\n",
    "            loss = tf.reduce_mean(tf.square(y - y_hat))\n",
    "            \n",
    "        if not best_loss or best_loss > loss.numpy():\n",
    "            best_loss = loss.numpy()\n",
    "        if best_loss < min_tol:\n",
    "            if verbose: print('terminated as loss less than minimal tolerance')\n",
    "            break\n",
    "            \n",
    "        if verbose and not (it % verbose): \n",
    "            print('mse loss at iteration {} is {:5.4f}'.format(it, loss))\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.variables)\n",
    "        for variables, grads in zip(model.variables, gradients):\n",
    "            variables.assign_add(-learning_rate * grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we recovered the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss at iteration 0 is 39.6477\n",
      "mse loss at iteration 1000 is 0.0280\n",
      "mse loss at iteration 2000 is 0.0015\n",
      "terminated as loss less than minimal tolerance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
       " array([[1.0662938],\n",
       "        [2.0163817],\n",
       "        [2.955478 ],\n",
       "        [4.037556 ],\n",
       "        [4.9171414]], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(x, y, model)\n",
    "model.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a bit complexity, we now add a bias term to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super(LinearModel, self).__init__(**kwargs)\n",
    "        self.w = tf.Variable(tf.random.uniform((input_dim, output_dim)), dtype=tf.float32)\n",
    "        self.b = tf.Variable(0, dtype=tf.float32)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss at iteration 0 is 45.3978\n",
      "mse loss at iteration 1000 is 0.1439\n",
      "mse loss at iteration 2000 is 0.0343\n",
      "mse loss at iteration 3000 is 0.0103\n",
      "mse loss at iteration 4000 is 0.0033\n",
      "mse loss at iteration 5000 is 0.0010\n",
      "terminated as loss less than minimal tolerance\n",
      "[<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[0.95385635],\n",
      "       [1.92548   ],\n",
      "       [2.9684424 ],\n",
      "       [3.9102137 ],\n",
      "       [4.9867525 ]], dtype=float32)>,\n",
      " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.13544115>]\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(input_dim=5, output_dim=1)    \n",
    "train(x, y, model)\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see the training code works just fine, with now the model has two sets of variables(as a result two sets of gradients). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer class  \n",
    "\n",
    "The `Model` class is usually for the overall computation which combines many repeated small computational modules. It is ok to code up the small modules with `tf.keras.Model` and then combine them, but its better to use the `Layer` class. \n",
    "\n",
    "Lets say that we want to upgrade the linear regression model to be a composition of two linear transformations. Here is how we utilize the `Layer` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, use_bias=True, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units))\n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(shape=(self.units), initializer=\"zeros\")\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        output = tf.matmul(x, self.w)\n",
    "        if self.use_bias:\n",
    "            output += self.b\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that with the `Layer` class, we actually creates the Variables in the `build` method, rather than the constructor. This method is called the first time there are data going through the layer, which allows dynamic determination of the sizes of the variables. This makes `layers` more ideal when you try to combining them. \n",
    "\n",
    "Now, lets stack up some layers to make a bigger model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(tf.keras.Model):\n",
    "    def __init__(self, num_units, use_bias=True, **kwargs):\n",
    "        super(LinearModel, self).__init__(**kwargs)\n",
    "        self.model = [Linear(units, use_bias) for units in num_units]\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the variables of the model before and after we pass some actual data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "---line break---\n",
      "[<tf.Variable 'linear_model_2/linear/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
      "array([[ 0.16933554, -0.5840391 ,  0.00741726],\n",
      "       [-0.3860661 , -0.43541566,  0.23082441],\n",
      "       [-0.28682524, -0.7829349 , -0.10946316],\n",
      "       [-0.8584915 ,  0.43169802,  0.07589662],\n",
      "       [ 0.26881176,  0.35085648,  0.16928047]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_2/linear/Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_2/linear_1/Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[ 0.193771 ],\n",
      "       [ 1.0162126],\n",
      "       [-1.2153293]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_2/linear_1/Variable:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n",
      "tf.Tensor(\n",
      "[[-1.1336187 ]\n",
      " [-0.91470057]\n",
      " [-1.1494504 ]\n",
      " [-0.7444345 ]\n",
      " [-0.8408779 ]], shape=(5, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(num_units=(3,1), use_bias=True)\n",
    "print(model.variables)\n",
    "print('---line break---')\n",
    "o = model(x)\n",
    "pprint(model.variables)\n",
    "print(o[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a model with many linear layers stacked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse loss at iteration 0 is 65.6646\n",
      "terminated as loss less than minimal tolerance\n",
      "[<tf.Variable 'linear_model_3/linear_2/Variable:0' shape=(5, 4) dtype=float32, numpy=\n",
      "array([[ 0.048112  , -0.27880564, -0.06568413, -0.03815908],\n",
      "       [ 0.27987158, -0.08447357,  0.67776585, -0.8038371 ],\n",
      "       [ 0.54184616, -0.5339778 , -0.4366298 , -0.6009581 ],\n",
      "       [-0.3520833 , -0.6955261 , -0.7057435 , -0.65394557],\n",
      "       [-0.15610582, -1.2245682 ,  0.72058666, -0.09791303]],\n",
      "      dtype=float32)>,\n",
      " <tf.Variable 'linear_model_3/linear_3/Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-0.24497788, -0.30466115,  0.24863297],\n",
      "       [-0.11622719, -1.2489358 , -0.50340974],\n",
      "       [ 0.24869917, -0.35284668,  0.66910475],\n",
      "       [ 0.694468  , -0.34810445, -0.77700156]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_3/linear_4/Variable:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-0.99073446,  0.46119884],\n",
      "       [-0.95910925, -1.1749245 ],\n",
      "       [-0.5191465 , -0.78819185]], dtype=float32)>,\n",
      " <tf.Variable 'linear_model_3/linear_5/Variable:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.68161815],\n",
      "       [-1.4929078 ]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model = LinearModel(num_units=(4,3,2,1), use_bias=False)\n",
    "train(x, y, model, learning_rate=.005)\n",
    "pprint(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem to be a bit more interesting than our old linear regression model at first glance. But the composition of linear transformations is still a linear transformation. With all these layers, we are essentially doing the same linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.1024506]\n",
      " [1.9742372]\n",
      " [2.975999 ]\n",
      " [3.9605384]\n",
      " [5.001549 ]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor(32.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "reduced_model = reduce(tf.matmul, model.variables)\n",
    "print(reduced_model)\n",
    "print(tf.reduce_sum(tf.cast((model(x) - x @ reduced_model) < 1e-5, tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many options for layers that are readily available from tensorflow, and there are pre-configured models too. We will get there in the future. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
